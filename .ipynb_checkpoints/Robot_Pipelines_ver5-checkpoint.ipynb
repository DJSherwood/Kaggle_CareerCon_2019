{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import logging\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from subprocess import check_output\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, GroupShuffleSplit\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import reciprocal, uniform, kurtosis, skew\n",
    "\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data (.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('X_train.csv')\n",
    "test = pd.read_csv('X_test.csv')\n",
    "y = pd.read_csv('y_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "\n",
    "This set of functions for feature engineering is taken from https://www.kaggle.com/jesucristo/1-smart-robots-most-complete-notebook. At first, I will simply use the functions to prepare my dataset. Later, I will see if I can add them to my pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/53033620/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\n",
    "def quaternion_to_euler(x, y, z, w):\n",
    "    import math\n",
    "    t0 = +2.0 * (w * x + y * z)\n",
    "    t1 = +1.0 - 2.0 * (x * x + y * y)\n",
    "    X = math.atan2(t0, t1)\n",
    "\n",
    "    t2 = +2.0 * (w * y - z * x)\n",
    "    t2 = +1.0 if t2 > +1.0 else t2\n",
    "    t2 = -1.0 if t2 < -1.0 else t2\n",
    "    Y = math.asin(t2)\n",
    "\n",
    "    t3 = +2.0 * (w * z + x * y)\n",
    "    t4 = +1.0 - 2.0 * (y * y + z * z)\n",
    "    Z = math.atan2(t3, t4)\n",
    "\n",
    "    return X, Y, Z\n",
    "\n",
    "def fe_step0 (actual):\n",
    "    \n",
    "    # https://www.mathworks.com/help/aeroblks/quaternionnorm.html\n",
    "    # https://www.mathworks.com/help/aeroblks/quaternionmodulus.html\n",
    "    # https://www.mathworks.com/help/aeroblks/quaternionnormalize.html\n",
    "    \n",
    "    # Spoiler: you don't need this ;)\n",
    "    \n",
    "    actual['norm_quat'] = (actual['orientation_X']**2 + actual['orientation_Y']**2 + actual['orientation_Z']**2 + actual['orientation_W']**2)\n",
    "    actual['mod_quat'] = (actual['norm_quat'])**0.5\n",
    "    actual['norm_X'] = actual['orientation_X'] / actual['mod_quat']\n",
    "    actual['norm_Y'] = actual['orientation_Y'] / actual['mod_quat']\n",
    "    actual['norm_Z'] = actual['orientation_Z'] / actual['mod_quat']\n",
    "    actual['norm_W'] = actual['orientation_W'] / actual['mod_quat']\n",
    "    \n",
    "    return actual\n",
    "\n",
    "def fe_step1 (actual):\n",
    "    \"\"\"Quaternions to Euler Angles\"\"\"\n",
    "    \n",
    "    x, y, z, w = actual['norm_X'].tolist(), actual['norm_Y'].tolist(), actual['norm_Z'].tolist(), actual['norm_W'].tolist()\n",
    "    nx, ny, nz = [], [], []\n",
    "    for i in range(len(x)):\n",
    "        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n",
    "        nx.append(xx)\n",
    "        ny.append(yy)\n",
    "        nz.append(zz)\n",
    "    \n",
    "    actual['euler_x'] = nx\n",
    "    actual['euler_y'] = ny\n",
    "    actual['euler_z'] = nz\n",
    "    \n",
    "    return actual\n",
    "\n",
    "def feat_eng(data):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 + data['angular_velocity_Z']**2)** 0.5\n",
    "    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 + data['linear_acceleration_Z']**2)**0.5\n",
    "    data['totl_xyz'] = (data['orientation_X']**2 + data['orientation_Y']**2 + data['orientation_Z']**2)**0.5\n",
    "    data['acc_vs_vel'] = data['totl_linr_acc'] / data['totl_anglr_vel']\n",
    "    \n",
    "    def mean_change_of_abs_change(x):\n",
    "        return np.mean(np.diff(np.abs(np.diff(x))))\n",
    "    \n",
    "    for col in data.columns:\n",
    "        if col in ['row_id','series_id','measurement_number']:\n",
    "            continue\n",
    "        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n",
    "        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n",
    "        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n",
    "        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n",
    "        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n",
    "        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n",
    "        df[col + '_maxtoMin'] = df[col + '_max'] / df[col + '_min']\n",
    "        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n",
    "        df[col + '_mean_change_of_abs_change'] = data.groupby('series_id')[col].apply(mean_change_of_abs_change)\n",
    "        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n",
    "        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n",
    "        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])/2\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _kurtosis(x):\n",
    "    return kurtosis(x)\n",
    "\n",
    "def CPT5(x):\n",
    "    den = len(x)*np.exp(np.std(x))\n",
    "    return sum(np.exp(x))/den\n",
    "\n",
    "def skewness(x):\n",
    "    return skew(x)\n",
    "\n",
    "def SSC(x):\n",
    "    x = np.array(x)\n",
    "    x = np.append(x[-1], x)\n",
    "    x = np.append(x,x[1])\n",
    "    xn = x[1:len(x)-1]\n",
    "    xn_i2 = x[2:len(x)]    # xn+1 \n",
    "    xn_i1 = x[0:len(x)-2]  # xn-1\n",
    "    ans = np.heaviside((xn-xn_i1)*(xn-xn_i2),0)\n",
    "    return sum(ans[1:]) \n",
    "\n",
    "def wave_length(x):\n",
    "    x = np.array(x)\n",
    "    x = np.append(x[-1], x)\n",
    "    x = np.append(x,x[1])\n",
    "    xn = x[1:len(x)-1]\n",
    "    xn_i2 = x[2:len(x)]    # xn+1 \n",
    "    return sum(abs(xn_i2-xn))\n",
    "    \n",
    "def norm_entropy(x):\n",
    "    tresh = 3\n",
    "    return sum(np.power(abs(x),tresh))\n",
    "\n",
    "def SRAV(x):    \n",
    "    SRA = sum(np.sqrt(abs(x)))\n",
    "    return np.power(SRA/len(x),2)\n",
    "\n",
    "def mean_abs(x):\n",
    "    return sum(abs(x))/len(x)\n",
    "\n",
    "def zero_crossing(x):\n",
    "    x = np.array(x)\n",
    "    x = np.append(x[-1], x)\n",
    "    x = np.append(x,x[1])\n",
    "    xn = x[1:len(x)-1]\n",
    "    xn_i2 = x[2:len(x)]    # xn+1\n",
    "    return sum(np.heaviside(-xn*xn_i2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_advanced_stats(data):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for col in data.columns:\n",
    "        if col in ['row_id','series_id','measurement_number']:\n",
    "            continue\n",
    "        if 'orientation' in col:\n",
    "            continue\n",
    "            \n",
    "        print (\"FE on column \", col, \"...\")\n",
    "        \n",
    "        df[col + '_skew'] = data.groupby(['series_id'])[col].skew()\n",
    "        df[col + '_mad'] = data.groupby(['series_id'])[col].mad()\n",
    "        df[col + '_q25'] = data.groupby(['series_id'])[col].quantile(0.25)\n",
    "        df[col + '_q75'] = data.groupby(['series_id'])[col].quantile(0.75)\n",
    "        df[col + '_q95'] = data.groupby(['series_id'])[col].quantile(0.95)\n",
    "        df[col + '_iqr'] = df[col + '_q75'] - df[col + '_q25']\n",
    "        df[col + '_CPT5'] = data.groupby(['series_id'])[col].apply(CPT5) \n",
    "        df[col + '_SSC'] = data.groupby(['series_id'])[col].apply(SSC) \n",
    "        df[col + '_skewness'] = data.groupby(['series_id'])[col].apply(skewness)\n",
    "        df[col + '_wave_lenght'] = data.groupby(['series_id'])[col].apply(wave_length)\n",
    "        df[col + '_norm_entropy'] = data.groupby(['series_id'])[col].apply(norm_entropy)\n",
    "        df[col + '_SRAV'] = data.groupby(['series_id'])[col].apply(SRAV)\n",
    "        df[col + '_kurtosis'] = data.groupby(['series_id'])[col].apply(_kurtosis) \n",
    "        df[col + '_zero_crossing'] = data.groupby(['series_id'])[col].apply(zero_crossing) \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This soooo needs a pipeline\n",
    "# train\n",
    "data = train\n",
    "\n",
    "data = fe_step0(data)\n",
    "data = fe_step1(data)\n",
    "data = feat_eng(data)\n",
    "data = fe_advanced_stats(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE on column  angular_velocity_X_mean ...\n",
      "FE on column  angular_velocity_X_median ...\n",
      "FE on column  angular_velocity_X_max ...\n",
      "FE on column  angular_velocity_X_min ...\n",
      "FE on column  angular_velocity_X_std ...\n",
      "FE on column  angular_velocity_X_range ...\n",
      "FE on column  angular_velocity_X_maxtoMin ...\n",
      "FE on column  angular_velocity_X_mean_abs_chg ...\n",
      "FE on column  angular_velocity_X_mean_change_of_abs_change ...\n",
      "FE on column  angular_velocity_X_abs_max ...\n",
      "FE on column  angular_velocity_X_abs_min ...\n",
      "FE on column  angular_velocity_X_abs_avg ...\n",
      "FE on column  angular_velocity_Y_mean ...\n",
      "FE on column  angular_velocity_Y_median ...\n",
      "FE on column  angular_velocity_Y_max ...\n",
      "FE on column  angular_velocity_Y_min ...\n",
      "FE on column  angular_velocity_Y_std ...\n",
      "FE on column  angular_velocity_Y_range ...\n",
      "FE on column  angular_velocity_Y_maxtoMin ...\n",
      "FE on column  angular_velocity_Y_mean_abs_chg ...\n",
      "FE on column  angular_velocity_Y_mean_change_of_abs_change ...\n",
      "FE on column  angular_velocity_Y_abs_max ...\n",
      "FE on column  angular_velocity_Y_abs_min ...\n",
      "FE on column  angular_velocity_Y_abs_avg ...\n",
      "FE on column  angular_velocity_Z_mean ...\n",
      "FE on column  angular_velocity_Z_median ...\n",
      "FE on column  angular_velocity_Z_max ...\n",
      "FE on column  angular_velocity_Z_min ...\n",
      "FE on column  angular_velocity_Z_std ...\n",
      "FE on column  angular_velocity_Z_range ...\n",
      "FE on column  angular_velocity_Z_maxtoMin ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gopherguy14/anaconda3/envs/kaggle_comps/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE on column  angular_velocity_Z_mean_abs_chg ...\n",
      "FE on column  angular_velocity_Z_mean_change_of_abs_change ...\n",
      "FE on column  angular_velocity_Z_abs_max ...\n",
      "FE on column  angular_velocity_Z_abs_min ...\n",
      "FE on column  angular_velocity_Z_abs_avg ...\n",
      "FE on column  linear_acceleration_X_mean ...\n",
      "FE on column  linear_acceleration_X_median ...\n",
      "FE on column  linear_acceleration_X_max ...\n",
      "FE on column  linear_acceleration_X_min ...\n",
      "FE on column  linear_acceleration_X_std ...\n",
      "FE on column  linear_acceleration_X_range ...\n",
      "FE on column  linear_acceleration_X_maxtoMin ...\n",
      "FE on column  linear_acceleration_X_mean_abs_chg ...\n",
      "FE on column  linear_acceleration_X_mean_change_of_abs_change ...\n",
      "FE on column  linear_acceleration_X_abs_max ...\n",
      "FE on column  linear_acceleration_X_abs_min ...\n",
      "FE on column  linear_acceleration_X_abs_avg ...\n",
      "FE on column  linear_acceleration_Y_mean ...\n",
      "FE on column  linear_acceleration_Y_median ...\n",
      "FE on column  linear_acceleration_Y_max ...\n",
      "FE on column  linear_acceleration_Y_min ...\n",
      "FE on column  linear_acceleration_Y_std ...\n",
      "FE on column  linear_acceleration_Y_range ...\n",
      "FE on column  linear_acceleration_Y_maxtoMin ...\n",
      "FE on column  linear_acceleration_Y_mean_abs_chg ...\n",
      "FE on column  linear_acceleration_Y_mean_change_of_abs_change ...\n",
      "FE on column  linear_acceleration_Y_abs_max ...\n",
      "FE on column  linear_acceleration_Y_abs_min ...\n",
      "FE on column  linear_acceleration_Y_abs_avg ...\n",
      "FE on column  linear_acceleration_Z_mean ...\n",
      "FE on column  linear_acceleration_Z_median ...\n",
      "FE on column  linear_acceleration_Z_max ...\n",
      "FE on column  linear_acceleration_Z_min ...\n",
      "FE on column  linear_acceleration_Z_std ...\n",
      "FE on column  linear_acceleration_Z_range ...\n",
      "FE on column  linear_acceleration_Z_maxtoMin ...\n",
      "FE on column  linear_acceleration_Z_mean_abs_chg ...\n",
      "FE on column  linear_acceleration_Z_mean_change_of_abs_change ...\n",
      "FE on column  linear_acceleration_Z_abs_max ...\n",
      "FE on column  linear_acceleration_Z_abs_min ...\n",
      "FE on column  linear_acceleration_Z_abs_avg ...\n",
      "FE on column  norm_quat_mean ...\n",
      "FE on column  norm_quat_median ...\n",
      "FE on column  norm_quat_max ...\n",
      "FE on column  norm_quat_min ...\n",
      "FE on column  norm_quat_std ...\n",
      "FE on column  norm_quat_range ...\n",
      "FE on column  norm_quat_maxtoMin ...\n",
      "FE on column  norm_quat_mean_abs_chg ...\n",
      "FE on column  norm_quat_mean_change_of_abs_change ...\n",
      "FE on column  norm_quat_abs_max ...\n",
      "FE on column  norm_quat_abs_min ...\n",
      "FE on column  norm_quat_abs_avg ...\n",
      "FE on column  mod_quat_mean ...\n",
      "FE on column  mod_quat_median ...\n",
      "FE on column  mod_quat_max ...\n",
      "FE on column  mod_quat_min ...\n",
      "FE on column  mod_quat_std ...\n",
      "FE on column  mod_quat_range ...\n",
      "FE on column  mod_quat_maxtoMin ...\n",
      "FE on column  mod_quat_mean_abs_chg ...\n",
      "FE on column  mod_quat_mean_change_of_abs_change ...\n",
      "FE on column  mod_quat_abs_max ...\n",
      "FE on column  mod_quat_abs_min ...\n",
      "FE on column  mod_quat_abs_avg ...\n",
      "FE on column  norm_X_mean ...\n",
      "FE on column  norm_X_median ...\n",
      "FE on column  norm_X_max ...\n",
      "FE on column  norm_X_min ...\n",
      "FE on column  norm_X_std ...\n",
      "FE on column  norm_X_range ...\n",
      "FE on column  norm_X_maxtoMin ...\n",
      "FE on column  norm_X_mean_abs_chg ...\n",
      "FE on column  norm_X_mean_change_of_abs_change ...\n",
      "FE on column  norm_X_abs_max ...\n",
      "FE on column  norm_X_abs_min ...\n",
      "FE on column  norm_X_abs_avg ...\n",
      "FE on column  norm_Y_mean ...\n",
      "FE on column  norm_Y_median ...\n",
      "FE on column  norm_Y_max ...\n",
      "FE on column  norm_Y_min ...\n",
      "FE on column  norm_Y_std ...\n",
      "FE on column  norm_Y_range ...\n",
      "FE on column  norm_Y_maxtoMin ...\n",
      "FE on column  norm_Y_mean_abs_chg ...\n",
      "FE on column  norm_Y_mean_change_of_abs_change ...\n",
      "FE on column  norm_Y_abs_max ...\n",
      "FE on column  norm_Y_abs_min ...\n",
      "FE on column  norm_Y_abs_avg ...\n",
      "FE on column  norm_Z_mean ...\n",
      "FE on column  norm_Z_median ...\n",
      "FE on column  norm_Z_max ...\n",
      "FE on column  norm_Z_min ...\n",
      "FE on column  norm_Z_std ...\n",
      "FE on column  norm_Z_range ...\n",
      "FE on column  norm_Z_maxtoMin ...\n",
      "FE on column  norm_Z_mean_abs_chg ...\n",
      "FE on column  norm_Z_mean_change_of_abs_change ...\n",
      "FE on column  norm_Z_abs_max ...\n",
      "FE on column  norm_Z_abs_min ...\n",
      "FE on column  norm_Z_abs_avg ...\n",
      "FE on column  norm_W_mean ...\n",
      "FE on column  norm_W_median ...\n",
      "FE on column  norm_W_max ...\n",
      "FE on column  norm_W_min ...\n",
      "FE on column  norm_W_std ...\n",
      "FE on column  norm_W_range ...\n",
      "FE on column  norm_W_maxtoMin ...\n",
      "FE on column  norm_W_mean_abs_chg ...\n",
      "FE on column  norm_W_mean_change_of_abs_change ...\n",
      "FE on column  norm_W_abs_max ...\n",
      "FE on column  norm_W_abs_min ...\n",
      "FE on column  norm_W_abs_avg ...\n",
      "FE on column  euler_x_mean ...\n",
      "FE on column  euler_x_median ...\n",
      "FE on column  euler_x_max ...\n",
      "FE on column  euler_x_min ...\n",
      "FE on column  euler_x_std ...\n",
      "FE on column  euler_x_range ...\n",
      "FE on column  euler_x_maxtoMin ...\n",
      "FE on column  euler_x_mean_abs_chg ...\n",
      "FE on column  euler_x_mean_change_of_abs_change ...\n",
      "FE on column  euler_x_abs_max ...\n",
      "FE on column  euler_x_abs_min ...\n",
      "FE on column  euler_x_abs_avg ...\n",
      "FE on column  euler_y_mean ...\n",
      "FE on column  euler_y_median ...\n",
      "FE on column  euler_y_max ...\n",
      "FE on column  euler_y_min ...\n",
      "FE on column  euler_y_std ...\n",
      "FE on column  euler_y_range ...\n",
      "FE on column  euler_y_maxtoMin ...\n",
      "FE on column  euler_y_mean_abs_chg ...\n",
      "FE on column  euler_y_mean_change_of_abs_change ...\n",
      "FE on column  euler_y_abs_max ...\n",
      "FE on column  euler_y_abs_min ...\n",
      "FE on column  euler_y_abs_avg ...\n",
      "FE on column  euler_z_mean ...\n",
      "FE on column  euler_z_median ...\n",
      "FE on column  euler_z_max ...\n",
      "FE on column  euler_z_min ...\n",
      "FE on column  euler_z_std ...\n",
      "FE on column  euler_z_range ...\n",
      "FE on column  euler_z_maxtoMin ...\n",
      "FE on column  euler_z_mean_abs_chg ...\n",
      "FE on column  euler_z_mean_change_of_abs_change ...\n",
      "FE on column  euler_z_abs_max ...\n",
      "FE on column  euler_z_abs_min ...\n",
      "FE on column  euler_z_abs_avg ...\n",
      "FE on column  totl_anglr_vel_mean ...\n",
      "FE on column  totl_anglr_vel_median ...\n",
      "FE on column  totl_anglr_vel_max ...\n",
      "FE on column  totl_anglr_vel_min ...\n",
      "FE on column  totl_anglr_vel_std ...\n",
      "FE on column  totl_anglr_vel_range ...\n",
      "FE on column  totl_anglr_vel_maxtoMin ...\n",
      "FE on column  totl_anglr_vel_mean_abs_chg ...\n",
      "FE on column  totl_anglr_vel_mean_change_of_abs_change ...\n",
      "FE on column  totl_anglr_vel_abs_max ...\n",
      "FE on column  totl_anglr_vel_abs_min ...\n",
      "FE on column  totl_anglr_vel_abs_avg ...\n",
      "FE on column  totl_linr_acc_mean ...\n",
      "FE on column  totl_linr_acc_median ...\n",
      "FE on column  totl_linr_acc_max ...\n",
      "FE on column  totl_linr_acc_min ...\n",
      "FE on column  totl_linr_acc_std ...\n",
      "FE on column  totl_linr_acc_range ...\n",
      "FE on column  totl_linr_acc_maxtoMin ...\n",
      "FE on column  totl_linr_acc_mean_abs_chg ...\n",
      "FE on column  totl_linr_acc_mean_change_of_abs_change ...\n",
      "FE on column  totl_linr_acc_abs_max ...\n",
      "FE on column  totl_linr_acc_abs_min ...\n",
      "FE on column  totl_linr_acc_abs_avg ...\n",
      "FE on column  totl_xyz_mean ...\n",
      "FE on column  totl_xyz_median ...\n",
      "FE on column  totl_xyz_max ...\n",
      "FE on column  totl_xyz_min ...\n",
      "FE on column  totl_xyz_std ...\n",
      "FE on column  totl_xyz_range ...\n",
      "FE on column  totl_xyz_maxtoMin ...\n",
      "FE on column  totl_xyz_mean_abs_chg ...\n",
      "FE on column  totl_xyz_mean_change_of_abs_change ...\n",
      "FE on column  totl_xyz_abs_max ...\n",
      "FE on column  totl_xyz_abs_min ...\n",
      "FE on column  totl_xyz_abs_avg ...\n",
      "FE on column  acc_vs_vel_mean ...\n",
      "FE on column  acc_vs_vel_median ...\n",
      "FE on column  acc_vs_vel_max ...\n",
      "FE on column  acc_vs_vel_min ...\n",
      "FE on column  acc_vs_vel_std ...\n",
      "FE on column  acc_vs_vel_range ...\n",
      "FE on column  acc_vs_vel_maxtoMin ...\n",
      "FE on column  acc_vs_vel_mean_abs_chg ...\n",
      "FE on column  acc_vs_vel_mean_change_of_abs_change ...\n",
      "FE on column  acc_vs_vel_abs_max ...\n",
      "FE on column  acc_vs_vel_abs_min ...\n",
      "FE on column  acc_vs_vel_abs_avg ...\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test = fe_step0(test)\n",
    "test = fe_step1(test)\n",
    "test = feat_eng(test)\n",
    "test = fe_advanced_stats(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"train_features.pkl\")\n",
    "test.to_pickle(\"test_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = pd.read_pickle(\"train_features.pkl\")\n",
    "test_feat = pd.read_pickle(\"test_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Zeros / NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace all 0 with Nan\n",
    "train_feat.replace(0, np.nan, inplace=True)\n",
    "train_feat.replace(-np.inf, np.nan, inplace=True)\n",
    "train_feat.replace(np.inf, np.nan, inplace=True)\n",
    "test_feat.replace(0, np.nan, inplace=True)\n",
    "test_feat.replace(-np.inf, np.nan, inplace=True)\n",
    "test_feat.replace(np.inf, np.nan, inplace=True)\n",
    "\n",
    "## Make list of columns which are all NaN\n",
    "def drop_missing(df):\n",
    "    p = ( df.isnull().sum() / df.isnull().count() ).sort_values(ascending=False)\n",
    "    keep_columns = list( p[ p < 0.06 ].index )\n",
    "    \n",
    "    return keep_columns\n",
    "\n",
    "train_col_to_keep = drop_missing(train_feat)\n",
    "test_col_to_keep = drop_missing(test_feat)\n",
    "master_col_to_keep = set(train_col_to_keep) & set(test_col_to_keep)\n",
    "\n",
    "final_train = train_feat[master_col_to_keep]\n",
    "final_test = test_feat[master_col_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Features to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train.to_pickle(\"final_train_features.pkl\")\n",
    "final_test.to_pickle(\"final_test_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'series_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/kaggle_comps/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'series_id'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f7471f3ca5ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'series_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'series_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'group_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m37\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_comps/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mset_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   4176\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4177\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4178\u001b[0;31m                 \u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4179\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4180\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_comps/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle_comps/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'series_id'"
     ]
    }
   ],
   "source": [
    "final_train = final_train.set_index('series_id').join(y.set_index('series_id'))\n",
    "trainb = final_train.loc[final_train['group_id'].isin([2,13,23,37,49])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the Reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_response=pd.DataFrame()\n",
    "instance_LabelEncoder = LabelEncoder()\n",
    "encoded_response['surface'] = instance_LabelEncoder.fit_transform(trainb['surface'])\n",
    "y_data = encoded_response['surface']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_depth': [12,15],\n",
    "    'n_estimators': [5, 10, 15],\n",
    "    'min_samples_leaf': [2,4]\n",
    "}\n",
    "             \n",
    "## Add summary transforms (calculate the mean, stdev, max, median, min) per group\n",
    "\n",
    "## Try a different cross validation method\n",
    "\n",
    "\n",
    "class_pipe = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42, sampling_strategy='minority')),\n",
    "    # ('add_features', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    ('standardize', StandardScaler()),\n",
    "    ('GrdSrch', GridSearchCV(RandomForestClassifier(), param_grid=parameters, scoring='accuracy'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define addtional variables in hopes of increasing readability\n",
    "X_data = trainb[['orientation_X','orientation_Y','orientation_Z','orientation_W','angular_velocity_X','angular_velocity_Y','angular_velocity_Z','linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z']]\n",
    "grp_data = trainb['group_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I think this will work\n",
    "name = [\"Random Forest Classifier\"]\n",
    "\n",
    "model_list = {}\n",
    "## Attach StratifiedKFold or whatever to CV\n",
    "#GrdSrch = GridSearchCV(class_pipe, parameters, verbose=2, scoring='accuracy')\n",
    "gs_classif = class_pipe.fit(X_data, y_data)\n",
    "    \n",
    "## Use cross_val_score\n",
    "score = cross_val_score(gs_classif, X=X_data, y=y_data, cv=GroupShuffleSplit().split(X=X_data, y=y_data, groups=grp_data))\n",
    "print(\"{} score: {}\".format(name, score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = encoded_response['surface']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_classif.named_steps['GrdSrch'].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Predictions\n",
    "y_hat = gs_classif.best_estimator_.predict(test[['orientation_X','orientation_Y','orientation_Z','orientation_W', \\\n",
    "                                                        'angular_velocity_X','angular_velocity_Y','angular_velocity_Z', \\\n",
    "                                                        'linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z']])\n",
    "\n",
    "## Transform back to labels\n",
    "test['surface'] = instance_LabelEncoder.inverse_transform(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try again with 250 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_response=pd.DataFrame()\n",
    "instance_LabelEncoder = LabelEncoder()\n",
    "encoded_response['surface'] = instance_LabelEncoder.fit_transform(train['surface'])\n",
    "y_data = encoded_response['surface']\n",
    "X_data = train[['orientation_X','orientation_Y','orientation_Z','orientation_W','angular_velocity_X','angular_velocity_Y','angular_velocity_Z','linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z']]\n",
    "\n",
    "\n",
    "## Final Pipe\n",
    "best_pipe = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42, sampling_strategy='minority')),\n",
    "    ('add_features', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
    "    ('standardize', StandardScaler()),\n",
    "    ('classify', RandomForestClassifier(max_depth=15, min_samples_leaf=2, min_samples_split=2, n_estimators=250))\n",
    "])\n",
    "\n",
    "# Run random forest on everything\n",
    "best_pipe.fit(X_data, y_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = best_pipe.predict(test[['orientation_X','orientation_Y','orientation_Z','orientation_W', \\\n",
    "                                                        'angular_velocity_X','angular_velocity_Y','angular_velocity_Z', \\\n",
    "                                                        'linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform back to labels\n",
    "test['surface'] = instance_LabelEncoder.inverse_transform(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join to submission file\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "answers = test.groupby('series_id').first()[['surface']]\n",
    "submission['surface'] = answers['surface']\n",
    "\n",
    "## save as csv\n",
    "submission.to_csv('submission_11Apr2019_djs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
